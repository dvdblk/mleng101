{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA3 visualized\n",
    "\n",
    "Note: requires about 40gb of RAM to run (on cpu by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load .env vars\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Warning: These weights are downloaded from official Meta\n",
    "# sources e.g. accepting TOS and using Meta's download.sh script.\n",
    "# (doesn't work with huggingface weights yet)\n",
    "LLAMA_WEIGHTS_PATH = Path(os.getenv(\"LLAMA_WEIGHTS_PATH\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"dim\": 4096,\n",
      "  \"n_layers\": 32,\n",
      "  \"n_heads\": 32,\n",
      "  \"n_kv_heads\": 8,\n",
      "  \"vocab_size\": 128256,\n",
      "  \"multiple_of\": 1024,\n",
      "  \"ffn_dim_multiplier\": 1.3,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 500000.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(LLAMA_WEIGHTS_PATH / \"params.json\", \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "    print(json.dumps(model_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the original model implementation w/o distributed thingies\n",
    "\n",
    "ofc could also do this by downloading the llama3 source repo code manually but this is a bit cooler etcetc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch.distributed\n",
    "import hiddenlayer as hl\n",
    "\n",
    "meta_llama_torch_url = \"https://raw.githubusercontent.com/meta-llama/llama3/32b7ddc9b638f0fd6b04b80e1845536a95f7a5d5/llama/model.py\"\n",
    "original_code = requests.get(meta_llama_torch_url).text\n",
    "\n",
    "def modify_code_for_cpu(code):\n",
    "# Replace parallel layers with PyTorch equivalents\n",
    "    code = code.replace(\"ColumnParallelLinear(\", \"nn.Linear(\")\n",
    "    code = code.replace(\"RowParallelLinear(\", \"nn.Linear(\")\n",
    "\n",
    "    # Special handling for VocabParallelEmbedding\n",
    "    code = code.replace(\n",
    "        \"VocabParallelEmbedding(\",\n",
    "        \"nn.Embedding(\"\n",
    "    )\n",
    "    code = code.replace(\"init_method=lambda x: x,\", \"\")\n",
    "    code = code.replace(\"gather_output=False,\", \"\")\n",
    "    code = code.replace(\"init_method=lambda x: x\", \"\")\n",
    "    code = code.replace(\"input_is_parallel=True,\", \"\")\n",
    "\n",
    "    # Remove model parallel initialization\n",
    "    code = code.replace(\"model_parallel_size = fs_init.get_model_parallel_world_size()\", \"# model_parallel_size = 1\")\n",
    "\n",
    "    # Adjust local heads calculation\n",
    "    code = code.replace(\"self.n_local_heads = args.n_heads // model_parallel_size\", \"self.n_local_heads = args.n_heads\")\n",
    "    code = code.replace(\"self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\", \"self.n_local_kv_heads = self.n_kv_heads\")\n",
    "\n",
    "    # Remove CUDA-specific code\n",
    "    code = code.replace(\".cuda()\", \"\")\n",
    "\n",
    "    # Add import for torch.nn\n",
    "    code = \"import torch.nn as nn\\n\" + code\n",
    "\n",
    "    return code\n",
    "\n",
    "# Modify the code\n",
    "modified_code = modify_code_for_cpu(original_code)\n",
    "exec(modified_code)\n",
    "\n",
    "original_llama_model = Transformer(ModelArgs(max_seq_len=2048, max_batch_size=32, **model_config)).bfloat16().to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchinfo.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Transformer                              [1, 16, 128256]           --\n",
      "├─Embedding: 1-1                         [1, 16, 4096]             525,336,576\n",
      "├─ModuleList: 1-2                        --                        --\n",
      "│    └─TransformerBlock: 2-1             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-1                 [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-2               [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-3                 [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-4             [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-2             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-5                 [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-6               [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-7                 [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-8             [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-3             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-9                 [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-10              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-11                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-12            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-4             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-13                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-14              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-15                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-16            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-5             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-17                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-18              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-19                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-20            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-6             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-21                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-22              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-23                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-24            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-7             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-25                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-26              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-27                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-28            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-8             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-29                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-30              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-31                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-32            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-9             [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-33                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-34              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-35                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-36            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-10            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-37                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-38              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-39                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-40            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-11            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-41                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-42              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-43                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-44            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-12            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-45                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-46              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-47                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-48            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-13            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-49                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-50              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-51                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-52            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-14            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-53                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-54              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-55                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-56            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-15            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-57                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-58              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-59                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-60            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-16            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-61                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-62              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-63                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-64            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-17            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-65                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-66              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-67                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-68            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-18            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-69                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-70              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-71                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-72            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-19            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-73                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-74              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-75                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-76            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-20            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-77                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-78              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-79                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-80            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-21            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-81                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-82              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-83                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-84            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-22            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-85                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-86              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-87                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-88            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-23            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-89                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-90              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-91                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-92            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-24            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-93                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-94              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-95                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-96            [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-25            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-97                [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-98              [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-99                [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-100           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-26            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-101               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-102             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-103               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-104           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-27            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-105               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-106             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-107               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-108           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-28            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-109               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-110             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-111               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-112           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-29            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-113               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-114             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-115               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-116           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-30            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-117               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-118             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-119               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-120           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-31            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-121               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-122             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-123               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-124           [1, 16, 4096]             176,160,768\n",
      "│    └─TransformerBlock: 2-32            [1, 16, 4096]             --\n",
      "│    │    └─RMSNorm: 3-125               [1, 16, 4096]             4,096\n",
      "│    │    └─Attention: 3-126             [1, 16, 4096]             41,943,040\n",
      "│    │    └─RMSNorm: 3-127               [1, 16, 4096]             4,096\n",
      "│    │    └─FeedForward: 3-128           [1, 16, 4096]             176,160,768\n",
      "├─RMSNorm: 1-3                           [1, 16, 4096]             4,096\n",
      "├─Linear: 1-4                            [1, 16, 128256]           525,336,576\n",
      "==========================================================================================\n",
      "Total params: 8,030,261,248\n",
      "Trainable params: 8,030,261,248\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 8.03\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 113.59\n",
      "Params size (MB): 16060.52\n",
      "Estimated Total Size (MB): 16174.11\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "tokens = [128000, 791, 5015, 574, 9960, 304, 264, 39139, 315, 4251, 11, 779, 433, 2011, 617, 1027]\n",
    "\n",
    "llama_summary = summary(original_llama_model, input_data=[torch.tensor(tokens).unsqueeze(0), 0], device=\"cpu\")\n",
    "print(llama_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchview graph\n",
    "\n",
    "Note: requires `graphviz` on the system ([https://stackoverflow.com/a/42875446/4249857](https://stackoverflow.com/a/42875446/4249857))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "graphviz.set_jupyter_format(\"png\")\n",
    "model_graph = draw_graph(\n",
    "    original_llama_model,\n",
    "    input_data=[torch.tensor(tokens).unsqueeze(0), 0],\n",
    "    device=\"cpu\",\n",
    "    depth=2,\n",
    "    roll=True\n",
    ")\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
